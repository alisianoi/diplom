\documentclass[12pt]{article}
\usepackage[a4paper]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}

\usepackage{mathtools, amssymb, bm}
\usepackage{IEEEtrantools}

\usepackage{minted}

\usepackage{tabu}
% `footnotes` fixes footnotes issues in tables
\usepackage{footnote}

\usepackage{graphicx}
\usepackage{subcaption}
% set up graphicx to use custom path to graphs
\graphicspath{{graphs/}}

% make indentation at the beginning of sections
\usepackage{indentfirst}

% backend=biber is the default but /wo it there is a warning
% use the biblatex-gost package which provides gost support
\usepackage[backend=biber,bibstyle=gost-standard]{biblatex}
\bibliography{references}

% set a couple mathematical environments
\newtheorem{definition}{Определение}
\newtheorem{example}{Пример}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage[norelsize,ruled,vlined]{algorithm2e}

% do not show colored borders around links
\usepackage[hidelinks]{hyperref}

% Параметры страницы
\renewcommand{\baselinestretch}{1.1}
%% \renewcommand{\baselinestretch}{1.5}

\begin{document}
\begin{titlepage}
\begin{center}
    Московский государственный университет имени М.\,В.~Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        ВЫПУСКНАЯ КВАЛИФИКАЦИОННАЯ РАБОТА \\СТУДЕНТА 417 ГРУППЫ\\[10mm]
        <<Обработка множеств логических закономерностей с помощью
        дисперсионного критерия>>
    }\\[10mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 4 курса 417 группы\\
            \emph{Лисяной Александр Евгеньевич}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., профессор\\
            \emph{Рязанов Владимир Васильевич}
        }
    \end{flushright}

    %% \begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}}
    %%     Заведующий кафедрой\newline
    %%     Математических Методов\newline
    %%     Прогнозирования, академик РАН
    %%     &
    %%     ~\newline~\newline
    %%     \hfill\hbox to 0.45\textwidth{\hrulefill~Ю. И. Журавлёв}
    %% \\[20mm]
    %%     К защите допускаю\newline
    %%     \hbox to 0.4\textwidth{<<\hbox to 12mm{\hrulefill}>> \hrulefill~2015 г.}
    %%     &
    %%     К защите рекомендую\newline
    %%     \hbox to 0.45\textwidth{
    %%       <<\hbox to 12mm{\hrulefill}>> \hrulefill~2015 г.
    %%     }
    %% \end{tabular}

    \vspace{\fill}
    Москва, 2015
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

%% \newpage
%% \begin{abstract}
%%   %% Данный документ является образцом оформления дипломной работы
%%   %% для студентов кафедры Математических методов прогнозирования
%%   %% ВМК~МГУ. Приведённые ниже рекомендации взяты из~статьи
%%   %% <<Написание отчётов и статей (рекомендации)>> на~вики"~ресурсе
%%   %% \texttt{www.MachineLearning.ru}.  Студенты, готовящие дипломную
%%   %% работу к~защите, могут найти много полезной информации также
%%   %% в~статьях <<Научно-исследовательская работа (рекомендации)>>,
%%   %% <<Подготовка презентаций (рекомендации)>>, <<Защита выпускной
%%   %% квалификационной работы (рекомендации)>> на~том~же ресурсе.

%%   %% Аннотация обычно содержит краткое описание постановки задачи
%%   %% и~полученных результатов, одним абзацем на 10--15 строк.  Цель
%%   %% аннотации "--- обозначить в~общих чертах, о~чём работа, чтобы
%%   %% человек, совершенно не~знакомый с~данной работой, понял,
%%   %% интересна~ли ему эта тема, и~стоит~ли читать дальше. Аннотация
%%   %% собирается в~последнюю очередь путем легкой модификации наиболее
%%   %% важных и~удачных фраз из введения и~заключения.

%%   Одной из основных задач машинного обучения является задача
%%   классификации. Существует множество разнообразных алгоритмов
%%   классификации, в том числе алгоритмы, основанные на поиске
%%   логических закономерностей в данных. Этот поиск --- довольно
%%   трудоемкая задача. Приходится прибегать к различного рода
%%   эвристикам, то есть как-то ограничивать время работы полного
%%   перебора. Полученные логические закономерности могут оказаться
%%   слишком сложными (переобученными) или очень похожими (не
%%   \emph{диверсифицированными}). Поэтому, как правило, применяются
%%   различные методы обработки множества логических
%%   закономерностей. Целью данной работы является реализовать и
%%   применить метод обработки множества логических закономерностей с
%%   помощью дисперсионного критерия к результату работы алгоритма
%%   <<логические закономерности>>, реализованного в системе
%%   <<Распознавание>>.

%% \end{abstract}

\newpage
\section{Введение}
%% Во введении рассказывается, где возникает данная задача, и~почему её
%% решение так важно. Вводится на неформальном уровне минимум терминов,
%% необходимый для понимания постановки задачи. Приводится краткий
%% анализ источников информации (литературный обзор): как эту задачу
%% решали до сих пор, в~чем недостаток этих решений, и~что нового
%% предлагает автор. Формулируются цели исследования. В~конце введения
%% даётся краткое содержание работы по разделам; при этом отмечается,
%% какие подходы, методы, алгоритмы предлагаются автором впервые.
%% При~упоминании ключевых разделов кратко формулируются основные
%% результаты и~наиболее важные выводы.

%% Цель введения: дать достаточно полное представление о~выполненном
%% исследовании и~полученных результатах, понятное широкому кругу
%% специалистов. Большинство читателей прочтут именно введение и, быть
%% может, заключение.

%% Во~введении автор решает сложную оптимизационную проблему: как
%% сообщить только самое важное, потратив минимум времени читателя,
%% да~так, чтобы максимум читателей поняли, о~чём вообще идёт речь.

%% Введение лучше писать напоследок, так как в~ходе работы обычно
%% происходит переосмысление постановки задачи.Если же введение писать,
%% когда работа еще не~готова, задача усложняется вдвойне.  В~конце
%% обычно приходит понимание, что всё получилось совсем не~так, как
%% планировалось в~начале, и~исходный вариант всё равно придётся
%% переписывать. Кстати, к~таким «потерям» надо относиться спокойно~---
%% в~хорошей работе почти каждый абзац многократно переделывается
%% до~неузнаваемости.

%% Введение имеет много общего с~текстом доклада на~защите, поэтому
%% имеет смысл готовить их одновременно.

Одной из основных задач машинного обучения является задача
классификации. Существует множество разнообразных алгоритмов
классификации: логистическая регрессия \cite{hosmer13}, метод опорных
векторов \cite{cortes95}, нейронные сети \cite{bishop95neural}. В этой
работе будут рассмотрены алгоритмы, основанные на поиске логических
закономерностей в данных \cites{ryazanov07logic, kovshov08,
  vainzvaig73kora}. Результатом работы таких алгоритмов является набор
логических закономерностей (правил), используя которые можно принимать
решение о том, к какому классу принадлежит тот или иной объект. Обычно
такие правила имеют вид конъюнкции по признакам объекта, где каждый
терм конъюнкции представляет из себя допустимый диапазон для значений
признака, при котором объект может быть \emph{принят} или
\emph{покрыт} данным правилом.

Поиск логических закономерностей в данных --- довольно трудоемкая
задача. Каждый алгоритм использует различные предположения при
построении логических закономерностей. Например, основная идея
алгоритма <<логические закономерности>>, реализованного в системе
<<Распознавание>> \cite{recognition06}, заключается в следующем: для
каждого объекта записывается дискретная оптимизационная задача,
решение которой однозначно определяет логическую закономерность. В
результате решения данных оптимизационных задач находятся необходимые
параметры логических закономерностей, определяющие левые и правые
границы допустимых диапазонов для каждого признака. В итоге получается
правило, которое как бы <<натянуто>> на некоторые объекты обучающей
выборки.

При использовании логических закономерностей возникают разного рода
трудности. Во-первых, множество логических закономерностей может быть
большим, тогда пользователю сложно интерпретировать это
множество. Во-вторых, множество логических закономерностей может
содержать похожие или даже вырожденные закономерности, что ухудшает
качество классификации с использованием этого множества.

Таким образом, отдельный интерес представляет задача по обработке
полученных множеств логических закономерностей. Эта задача заключается
в том, чтобы по исходному множеству логических закономерностей
построить множество меньшей мощности, что должно упростить
пользователю задачу интерпретации полученных правил. Вторая цель при
обработке заключается в том, чтобы построенное множество логических
закономерностей меньшей мощности имело качество классификации,
сравнимое с исходным множеством.

Целью данной работы является реализовать и применить метод обработки
множества правил, основанный на кластеризации с помощью дисперсионного
критерия, к результату работы алгоритма <<логические закономерности>>,
представленного в системе <<Распознавание>>. В этой работе
предлагается использовать описание логических закономерностей с
помощью вектора левых и правых границ. Также целью работы является
сравнить этот метод с бинаризованным описанием логических
закономерностей, предложенным в \cite{novikov15}.

Обработанные множества логических закономерностей сравниваются как
между собой (в ходе обработки используются различные способы описания
правил и критерии информативности), так и с исходным множеством
логических закономерностей, полученным в результате настройки
алгоритма системы <<Распознавания>>. В результате предложенный в
данной работе подход с использованием вектора левых и правых границ в
целом работает лучше, чем подход с бинаризованным описанием из
\cite{novikov15}. На основе относительно большого числа исходных
правил вычисляется существенно меньшее число правил. При их
использовании по схеме простого голосования получается классификатор,
качество которого сравнимо с качеством алгоритма <<логические
закономерности>> из системы <<Распознавание>>.

\section{Задача поиска логических закономерностей}
%% Лучше, чтобы название этого подраздела было содержательным, например,
%% общепринятым названием задачи, проблемы или метода, рассматриваемого
%% в~данной работе.

%% Перечисляются подходы, методы, факты, на которые существенно опирается
%% данная работа, но~которые могут быть не~известны широкому кругу
%% читателей.  Здесь ссылки на литературу обязательны.  Теоремы только
%% формулируются, но не~доказываются.

%% Данный раздел преследует две цели.  Во-первых, сделать работу
%% самодостаточной~--- дать необходимый минимум информации тем читателям,
%% которые не~очень хорошо ориентируются в~теме, но желают поближе
%% познакомиться именно с~данной работой.  Во-вторых, облегчить
%% сопоставление полученных автором результатов с~ранее известными.

\subsection{Определения и~обозначения}
\label{subsec:defs}

%% Формальная постановка задачи. Для известных понятий желательно
%% придерживаться стандартных обозначений.

%% Общепринятые термины вводятся словом «называется». Термины,
%% придуманные самим автором, вводятся словами «назовём» или «будем
%% называть».  Обычно этот раздел заканчивается формальной постановкой
%% задачи.  Именно с~этого раздела стоит начинать писать работу.

% постановка задачи классификации Воронцовым
Основные определения и обозначения будем вводить согласно
\cite{voron10logicalgs}. Пусть имеется пространство объектов \(X\) и
конечное множество имен классов \(Y = \left\{1, \dots,
M\right\}\). Пусть также имеется \emph{обучающая выборка} \(X^{l} =
(x_i, y_i)_{i = 1}^{l}\), в которой для каждого объекта \(x_i\)
известен его класс \(y_i \in Y\).  Для восстановления целевой
зависимости \(y^{*}(x_i) = y_i\) построим алгоритм классификации
\(a\colon X \rightarrow Y\), аппроксимирующий \(y^{*}\) на всём
пространстве объектов \(X\).

Для решения такой задачи классификации (восстановления зависимости
\(y^{*}\) по обучающей выборке \(X^l\)) существует множество
разнообразных алгоритмов. Некоторые из них основаны на поиске
логических закономерностей.

\begin{definition}
  Функция \(\varphi(x) \colon X \rightarrow \left\{0, 1\right\}\)
  называется \emph{предикатом}. Говорят, что предикат
  \(\varphi\) \emph{выделяет} или \emph{покрывает} объект \(x \in X\),
  если \(\varphi(x) = 1\).
\end{definition}

Формализуем понятие \emph{закономерности}. Для этого введем несколько
обозначений:

\begin{itemize}
\item Пусть \(P_c\) --- число объектов класса \(c\) в выборке \(X^l\).
\item Пусть \(p_c(\varphi)\) --- число объектов \(x\) класса \(c\), на
  которых \(\varphi(x) = 1\)
\item Пусть \(N_c\) --- число объектов всех остальных классов \(Y
  \setminus \left\{c\right\}\)
\item Пусть \(n_c(\varphi)\) --- число объектов \(x\) не из объектов
  класса \(c\), на которых \(\varphi(x) = 1\)
\item Обозначим долю ошибочно выделяемых объектов среди всех объектов,
  покрываемых \emph{закономерностью}, через \(E_c(\varphi, X^l) =
  \frac{n_c(\varphi)}{p_c(\varphi) + n_c(\varphi)}\)
\item Обозначим долю верно выделяемых объектов через \(D_c(\varphi,
  X^l) = \frac{p_c(\varphi)}{l}\)
\end{itemize}

\begin{definition}
  Предикат \(\varphi(x)\) называется
  \emph{\(\varepsilon\),\(\delta\)-закономерностью} для класса \(c \in
  Y\), если \(E_c(\varphi, X^l) \leq \varepsilon \) и \(D_c(\varphi,
  X^l)\geq\delta\) при заданных \(\varepsilon\) и \(\delta\) из
  отрезка \([0, 1]\)
\end{definition}

Такие \(\varepsilon\),\(\delta\)-закономерности также называют просто
\emph{закономерностями} или \emph{правилами}. Закономерность
называется \emph{чистой} или \emph{непротиворечивой}, если она не
покрывает объекты чужого класса. В противном случае закономерность
называется \emph{частичной}

Другой способ определения понятия \emph{закономерности} основан на
понятии \emph{энтропии}. Будем считать появление объекта класса \(c\)
исходом \(\omega_0\), а появление объекта любого другого класса ---
исходом \(\omega_1\). Вспомним, что функция энтропии для дискретной
случайной величины с двумя исходами, вероятности которых \(p \text{ и
} q = 1 - p\), имеет вид (основание логарифма выбрано равным \(2\)):

\[\mathbb{H}(p, q) = -p\log_2(p)-q\log_2(q)\]

Пусть далее в выборке \(X^l\) имеется \(P_c\) объектов из класса \(c\)
и \(N_c\) объектов не из класса \(c\) (таким образом, \(P_c + N_c =
l\)). Тогда можно оценить энтропию выборки по формуле:

\begin{equation}\label{eq:entropy:approx}
\tilde{\mathbb{H}}(P_c, N_c) =
\mathbb{H}\left(\frac{P_c}{P_c+N_c},\frac{N_c}{P_c+N_c}\right)
\end{equation}

Когда предикат \(\varphi(x)\) покрывает \(p_c\leq P_c\) объектов из
класса \(c\) и \(n_c\leq N_c\) объектов не из класса \(c\), то он
разбивает исходную выборку на две подвыборки: \(\left\{x\in X^l |
\varphi(x) = 1\right\}\) и \(\left\{x\in X^l | \varphi(x) =
0\right\}\), энтропии которых можно оценить аналогично
\ref{eq:entropy:approx} как \(\tilde{\mathbb{H}}(p_c, n_c)\) и
\(\tilde{\mathbb{H}}(P_c - p_c, N_c - n_c)\)
соответственно. Вероятность того, что объект принадлежит одной из
подвыборок, оценивается, как \(\frac{p_c + n_c}{P_c + N_c}\) и
\(\frac{P_c + N_c - p_c - n_c}{P_c + N_c}\) соответственно. Тогда
после получения информации от предиката \(\varphi(x)\) энтропия всей
выборки оценивается так:

\[
\tilde{\mathbb{H}}(P_c, N_c, p_c, n_c) =
\frac{p_c + n_c}{P_c + N_c}\tilde{\mathbb{H}}(p_c, n_c) +
\frac{P_c+N_c-p_c-n_c}{P_c+N_c}\tilde{\mathbb{H}}(P_c - p_c, N_c - n_c)
\]

Уменьшение энтропии при этом составляет:
\[
\text{IGain}_c(\varphi, X^l) =
\tilde{\mathbb{H}}(P_c, N_c) - \tilde{\mathbb{H}}(P_c, N_c, p_c, n_c)
\]

Эта величина называется \emph{информационным выигрышем} и отражает
количество информации, которое содержится в предикате
\(\varphi(x)\). Это позволяет ввести альтернативное определение
\emph{закономерности}:

\begin{definition}
  Предикат \(\varphi(x)\) называется \emph{закономерностью} по
  энтропийному критерию информативности, если \(IGain_c (\varphi, X^l)
  > G_0\) при некотором \(G_0\).
\end{definition}

Вообще говоря, предикаты \(\varphi(x)\colon X \rightarrow Y\) могут
иметь довольно сложный вид. Например, если в качестве объектов из
\(X\) выступают точки на числовой прямой, то для классификации на
рациональные и иррациональные числа можно взять аналог функции Дирихле
в качестве предиката:

\[
\varphi(x) =
\begin{dcases*}
1 & \(x \in \mathbf{Q}\) \\
0 & \(x \in \mathbf{R} \setminus \mathbf{Q}\)
\end{dcases*}
\]

Для того, чтобы предикаты были более простыми, введем
ограничение на их вид так, как было сделано в
\cite{ryazanov07logic}.

\begin{definition}
Пусть \(c_1, c_2 \in R\), \(f_j(x)\) --- j-ый признак объекта \(x\),
тогда \emph{параметрическим элементарным предикатом} будем
называть
\[
P^{c_1, c_2}(f_j(x)) =
\begin{dcases*}
1 & при \(c_1 \leq f_j(x) \leq c_2\) \\
0 & иначе
\end{dcases*}
\]
\end{definition}

Поскольку в предикате используется лишь один признак из всего
признакового описания объекта, то такой предикат называется
элементарным. Параметры \(c_1, c_2 \in R\) для каждого отдельного
признака \(f_j(x)\) выбираются, вообще говоря, свои, поэтому предикат
называют параметрическим.

\begin{definition}
  \label{def:parpred}
  Пусть каждый объект выборки \(x\in X^l\) имеет размерность \(D\) и
  пусть \(\Omega\subseteq\left\{1, 2, \dots, D\right\}\). Предикат
  \[
  \varphi(x) = P^{\Omega, \bm{c_1}, \bm{c_2}}(x) =
  \bigwedge_{j\in\Omega}P^{c_1^j, c_2^j}(f_j(x))
  \]
  называется логической закономерностью класса \(c\), если выполнено:
  \begin{enumerate}
  \item \(\exists x\in c\colon \varphi(x) = 1\)
  \item \(\forall x\not\in c\colon \varphi(x) = 0\)
  \item \(\varphi(x) =
    \argmax_{\varphi^*(x)}\Phi(\varphi^*(x))\), где \(\Phi\)
    --- критерий качества предиката.
  \end{enumerate}
\end{definition}

Если предикат удовлетворяет только первым двум условиям, то предикат
\(\varphi(x)\) называется \emph{допустимым}. Если выполнены все
условия, кроме второго, то такая логическая закономерность называется
\emph{частичной}. Индексы \(j\) над параметрами \(c_1^j, c_2^j\)
означают, что для каждого признака могут быть выбраны свои
границы. Для логических закономерностей такого вида есть простая
геометрическая интерпретация --- это построенные по объектам обучающей
выборки \(X\) прямоугольные гиперпараллелепипеды. Пример для случая
объектов с двумя признаками на рисунке
\ref{fig:possibilities}. Стандартным критерием качества согласно
\cite{kovshov08} и \cite{ryazanov07logic} называется:
\[
\Phi(\varphi(x)) = |\left\{x\in c\colon \varphi(x) = 1\right\}|
\]

\begin{figure}[!htbp]
  \centering
  \includegraphics
      [width=0.5\textwidth,keepaspectratio]
      {boundary_choice}
      \caption{
        Два класса (фиолетовые круги и зеленые квадраты) и область, в
        которой можно построить логическую закономерность
        (диагональная штриховка).
      }
      \label{fig:possibilities}
\end{figure}

\subsection{Методы поиска логических закономерностей}
Пусть множество \(\mathcal{P}\) параметрических элементарных
предикатов конечно. Тогда множество \(\mathcal{K}_K\) логических
закономерностей, состоящих из \(K\) термов, имеет вид:

\[
\mathcal{K}_K =
\left\{
 \varphi(x) = P_1(x) \& P_2(x) \& \dots \& P_K(x) \mid
  P_1, \dots, P_K \in \mathcal{P}
\right\}
\]

Количество логических закономерностей: \(\lvert \mathcal{K}_K \rvert =
\lvert \mathcal{P} \rvert^{K}\). Существует большое количество
алгоритмов, которые по-разному подходят к построению множества
логических закономерностей. К таким алгоритмам относятся <<КОРА>>
\cite{vainzvaig73kora}, <<IREP>> и <<RIPPER>> \cite{cohen95fast},
<<SLIPPER>> \cite{cohen99simple}, <<ID3>> \cite{quinlan86induction},
<<C4.5rules>> \cite{quinlan93programs}, \cite{quinlan96bagging}.  В
качестве классического примера \ref{algo:IREP} такого алгоритма можно
привести двухклассовую версию <<IREP>>. Это жадный алгоритм построения
правил, работающий по принципу <<разделяй и властвуй>> (divide and
conquer). Этот принцип проявляется в том, что правила создаются по
одному, а объекты, которые покрываются созданным правилом, исключаются
из рассмотрения. Многие алгоритмы (<<КОРА>>, <<ТЭМП>>
\cite{voron10logicalgs}) по смыслу очень похожи и отличаются только
критериями останова и способами обработки полученных правил.

\begin{algorithm}[\text]
  \caption{Incremental Reduced Error Pruning (IREP)}\label{algo:IREP}
  \SetKwInOut{Input}{Вход}\SetKwInOut{Output}{Выход}
  \Input{Pos --- множество элементов класса \(+1\) \newline
    Neg --- множество элементов класса \(-1\)
  }
  \Output{Rules --- множество правил}
  \SetKwFunction{IREP}{IREP}
  \SetKwProg{Fn}{Функция}{}{}
  \SetKwFor{While}{Пока}{}{}
  \SetKwIF{If}{Elif}{Else}{Если}{то}{Иначе если}{Иначе}{}
  \SetKw{KwRet}{Вернуть}
  \Fn{\IREP{Pos, Neg}} {
    Завести пустой набор правил Ruleset\;
    \While{Pos не опустеет}{
      \tcc{Нужно обучить и упростить очередное правило Rule}
      Разделить (Pos, Neg) на (GrowPos, GrowNeg) и (PrunePos, PruneNeg)\;
      Обучить правило Rule по паре множеств (GrowPos, GrowNeg)\;
      Упростить правило Rule по паре множеств (PrunePos, PruneNeg)\;
      \uIf{доля ошибок Rule на (PrunePos, PruneNeg) больше 50\%}{
        \KwRet{Ruleset}\;
      }
      \Else{
        Добавить правило Rule в Ruleset\;
        Убрать из (Pos, Neg) объекты, которые покрывает правило Rule\;
      }
    }
    \KwRet{Ruleset}\;
  }
\end{algorithm}

В системе <<Распознавание>> реализован алгоритм <<логические
закономерности>>, который подробно описан в работах \cite{kovshov08},
\cite{ryazanov07logic}, \cite{recognition06}. Основная идея этого
алгоритма заключается в следующем: для каждого объекта записывается
дискретная оптимизационная задача, решение которой однозначно
определяет логическую закономерность. В результате решения данных
оптимизационных задач находятся необходимые параметры логических
закономерностей, определяющие левые и правые границы интервалов. В
итоге получается правило, которое как бы <<натянуто>> на некоторые
объекты обучающей выборки. Точный комбинаторный метод основан на
методе ветвей границ.

Многие алгоритмы поиска логических закономерностей имеют следующие
недостатки:

\begin{enumerate}
\item Количество найденных правил может быть довольно большим. Из-за
  этого пользователю может быть сложно интерпретировать эти правила.
\item Найденные логические закономерности могут получиться очень
  похожими или даже вырожденными. В случае алгоритма из системы
  <<Распознавание>> это связано с близостью решаемых оптимизационных
  задач.
\end{enumerate}

Первая проблема ставит интересную задачу поиска множества логических
закономерностей, которое содержит меньшее число элементов и при этом
имеет качество классификации, которое хотя бы не хуже, чем у исходного
множества правил. Вторая проблема хорошо иллюстрируется примером
\ref{ex:same:experts}, взятом из \cite{voron10logicalgs}.

\begin{example}\label{ex:same:experts}
  Предположим, что в некоторой экспертной комиссии есть два эксперта,
  мнения которых всегда (или почти всегда) совпадают по всем
  вопросам. Интуитивно понятно, что если убрать одного из экспертов,
  то качество работы новой комиссии будет таким же (или почти таким
  же).
\end{example}

Поэтому понятно, что способ обработки логических закономерностей
должен по-возможности оставлять различные правила. Наконец, от
вырожденных правил, которые покрывают малое число объектов (например,
один) стоит и вовсе избавляться.

\section{Обработка логических закономерностей}
\label{sec:processing}
%% Название этого раздела обязательно надо заменить на~содержательное.
%% В~этом разделе, как правило, много подразделов.

%% В~дипломной работе не~стоит делать более двух уровней, достаточно
%% разделов и~подразделов. Будете писать диссертацию или монографию~---
%% сделаете три уровня.

Пусть в результате работы некоторого метода поиска логических
закономерностей было получено множество правил \(\left\{\varphi_1(x),
\dots, \varphi_n(x)\right\}\). Для того, чтобы обработать это
множество, предлагается провести его кластеризацию с помощью
дисперсионного критерия. Основная схема этого подхода, предложенного
В.\,В.~Рязановым, выглядит следующим образом:

\begin{enumerate}
\item Каждой логической закономерности \(\varphi_j^i\) из множества
  \(
  \mathcal{K}_{i} = \left\{
  \varphi_1^i(x), \dots, \varphi_t^i(x)
  \right\}
  \)
  логических закономерностей, покрывающих класс \(y_i\), поставить в
  соответствие некоторый вектор \(\bm{z}_j\), описывающий эту
  закономерность.
\item Провести кластеризацию векторов \(\bm{z}_1, \dots, \bm{z}_t\) на
  \(k \leq t\) кластеров и найти центры этих кластеров
  \(\bm{z}_1^*, \dots, \bm{z}_k^*\).
\item По центрам кластеров восстановить \(\varphi_1^*(x), \dots,
  \varphi_k^*(x)\) --- некоторые, вообще говоря новые, логические
  закономерности для класса \(y_i\).
\end{enumerate}

\subsection{Способы представления логических закономерностей}
\label{subsec:representation}
В этой работе рассмотрено два способа представить логические
закономерности: с помощью бинарного вектора и с помощью вектора левых
и правых границ.

Представление с помощью бинарного вектора было исследовано в
\cite{novikov15}. Идея заключается в том, чтобы каждой логической
закономерности \(\varphi(x)\) поставить в соответствие вектор
\(\bm{z}\in \left\{0, 1\right\}^l\) так, что

\[
\bm{z}_i =
\begin{dcases*}
1 & если \(\varphi(x_i) = 1\) \\
0 & иначе
\end{dcases*}
\]

В данной работе предлагается использовать представление с помощью
вектора левых и правых границ, которое использует введенное в
\ref{subsec:defs} представление логической закономерности
\(\varphi(x)\) в виде параметрических элементарных предикатов
(\ref{def:parpred}). При таком подходе вектор \(\bm{z}\) принимает вид:

\[\bm{z} = (c_1^1, c_2^1, c_1^2, c_2^2, \dots, c_1^D, c_2^D)\]

\subsection{Алгоритм кластеризации логических закономерностей}
Нужно разбить полученное описание множества логических
закономерностей \(\bm{z}_1, \bm{z}_2, \dots, \bm{z}_t\) на \(k \leq
t\) непересекающихся кластеров \(\bm{S} = \left\{S_1, S_2, \dots,
S_k\right\}\) таким образом, чтобы минимизировать функционал
\ref{eq:quality}.

\begin{equation}\label{eq:quality}
\bm{S}^* =
\argmin_{\bm{S}}
\sum_{i=1}^k \sum_{\bm{z}_j\in S_i} \|\bm{z}_j - \bm{\mu}_i\|^2
\end{equation}

Тогда алгоритм кластеризации
(алгоритм Ллойда \cite{lloyd06}, алгоритм К-средних \cite{macqueen67})
примет вид алгоритма \ref{algo:cluster}.

\begin{algorithm}[!htpb]
  \caption{Алгоритм кластеризации (Ллойда, К-средних)}
  \label{algo:cluster}
  \SetKwInOut{Input}{Вход}\SetKwInOut{Output}{Выход}
  \Input{Описание закономерностей \(\bm{z}_1, \dots, \bm{z}_t\)}
  \Output{
    \(\bm{S}^* =S_1, \dots, S_k\) --- разбиение на непересекающиеся
    кластеры
  }
  \SetKwFunction{IREP}{IREP}
  \SetKwProg{Fn}{Функция}{}{}
  \SetKwFor{While}{Пока}{}{}
  \SetKwIF{If}{Elif}{Else}{Если}{то}{Иначе если}{Иначе}{}
  \SetKw{KwRet}{Вернуть}
  \Fn{Кластеризовать(\(\bm{z}_1, \dots, \bm{z}_t\))} {
    Инициализировать центры кластеров
    \(\bm{\mu}_1^1, \dots, \bm{\mu}_k^1\)\;
    \While{кластеризация не стабилизируется}{
      \tcc{распределить объекты по кластерам, при этом}
      \tcc{минимизировать функционал качества \ref{eq:quality}}
      \(
      S_i^t = \left\{
      \bm{z}_p : \|\bm{z}_p - \bm{\mu}_i^t\|^2
      \leq
      \|\bm{z}_p - \bm{\mu}_j^t\|^{2}, \forall j: 1\leq j\leq k
      \right\}
      \)\;
      \tcc{пересчитать центры кластеров}
      \(
      \bm{\mu}_i^{t+1} =
      \frac{1}{|S_i^t|}
      \frac{
        \sum_{\bm{z}_j\in S_i^t}\beta_j \bm{z}_j
      }{
        \sum_{\bm{z}_j\in S_i^t}\beta_j
      }
      \)\;
    }
    \KwRet{\(S_1^t, \dots, S_k^t\)}\;
  }
\end{algorithm}

\subsection{Построение логических закономерностей по результату кластеризации}
После того, как алгоритм кластеризации находит разбиение
\(\bm{S}^*\), необходимо по центрам кластеров
\(\bm{\hat{z}}_1^*, \dots, \bm{\hat{z}}_k^*\)
построить логические закономерности
\(\varphi_1^*(x), \dots, \varphi_k^*(x)\).

В случае, когда логические закономерности исходно были представлены в
виде бинарных векторов, полученный центр кластера \(\bm{\hat{z}}_i^*\)
уже, вообще говоря, не является бинарным вектором. Поэтому для каждого
вектора \(\bm{\hat{z}}^*_i, i = 1,\dots, k\) выбирается некоторый порог
бинаризации \(\theta_i\) таким образом, чтобы логическая закономерность
\(\varphi_i^*(x)\), отвечающая представлению:

\[
\bm{z}_i^* =
\begin{dcases*}
1 & если \(\bm{\hat{z}}_i^* \geq \theta_i\) \\
0 & иначе
\end{dcases*}
\]

была наиболее информативной среди всех рассматриваемых значений порога
\(\theta_i\). Информативность логической закономерности рассчитывается
с помощью известных критериев информативности, таких как энтропийный
критерий IGain. Подробный обзор использования разных критериев для
этой задачи представлен в \cite{novikov15}.

В случае, когда логические закономерности исходно описывались с
помощью вектора левых и правых границ, полученный центр кластера
\(\bm{\hat{z}}_i^*\) можно напрямую использовать в качестве описания
\(\bm{z}_i^*\) некоторой новой логической закономерности. То есть нет
необходимости проводить отдельную процедуру по восстановлению
описания.

\section{Вычислительные эксперименты}
\label{sec:experiments}
%% Цель данного раздела: продемонстрировать, что предложенная теория
%% работает на практике; показать границы её применимости; рассказать
%% о~новых экспериментальных фактах.

Основной целью вычислительных экспериментов было реализовать подход к
обработке множества логических закономерностей с помощью
кластеризации по дисперсионному критерию, описанного в разделе
\ref{sec:processing}. Второй целью вычислительного эксперимента было
сравнить способы представления логических закономерностей, описанные в
разделе \ref{subsec:representation}. Наконец, третьей целью
вычислительного эксперимента было понять, возможно ли с помощью
описанного подхода получить множество правил с меньшим числом
элементов и сравнимым качеством классификации.

\subsection{Исходные данные и условия экспериментов}
%% Описывается прикладная задача, параметры анализируемых данных
%% (например, сколько объектов, сколько признаков, каких они типов),
%% параметры эксперимента (например, как производился скользящий
%% контроль).

Исходные данные были взяты из репозитория UCI \cite{Lichman2013}.
Ниже представлена сводная таблица \ref{tab:data} по использованным
данным.

\begin{savenotes}
\begin{table}[!htbp]
  \begin{tabu}{X X[2] X[2] X}
    Выборка & Всего объектов & Объекты по классам & Признаки \\ \hline
    Iris\footnotemark[1] & 150 & 50/50/50 & 4   \\
    Wine\footnotemark[2] & 178 & 59/71/48 & 13  \\
    Climate\footnotemark[3] & 540 & 46/494   & 11  \\
    Ionosphere\footnotemark[4] & 351 & 126/255  & 34  \\
  \end{tabu}
  \caption{Сводная таблица по использованным данным}
  \label{tab:data}
\end{table}
\end{savenotes}

Выборка Iris\footnotemark[1] ставит задачу классификации растений
семейства ирисов. Выборка Wine\footnotemark[2] содержит данные
химического анализа вина, полученного от трех разных итальянских
виноделов. Выборка Climate\footnotemark[3] содержит информацию об
удачных и неудачных запусках симулирования погоды. Наконец, выборка
Ionosphere\footnotemark[4] исследует свободные электроны в ионосфере.

\footnotetext[1]{\url{http://archive.ics.uci.edu/ml/datasets/Iris}}
\footnotetext[2]{\url{http://archive.ics.uci.edu/ml/datasets/Wine}}
\footnotetext[3]{
  \url{
    https://archive.ics.uci.edu/ml/datasets/Climate+Model+Simulation+Crashes
  }
}
\footnotetext[4]{\url{https://archive.ics.uci.edu/ml/datasets/Ionosphere}}

\subsection{Результаты экспериментов}
%% Результаты экспериментов представляются в~виде таблиц и~графиков.
%% Объясняется точный смысл всех обозначений на графиках, строк
%% и~столбцов в~таблицах.

Для выполнения основной цели вычислительного эксперимента было сделано
следующее:
\begin{enumerate}
\item Был найден способ работать в системе <<Распознавание>> под
  операционной системой семейства Linux. Детали смотрите в Приложении
  А.
\item Система <<Распознавание>> использует собственный формат файла
  для исходных выборок с данными. Было найдено описание этого формата и
  написана программа по преобразованию исходных выборок в этот
  формат. Детали смотрите в Приложении Б.
\item Результаты работы алгоритма <<логические закономерности>>
  системы <<Распознавание>> представляются в виде отчетов. Была
  написана программа для извлечения построенных правил из этих
  отчетов.
\item Для реализации подхода к обработке логических закономерностей
  была написана программа, которая:
  \begin{enumerate}
  \item Представляет построенные правила в виде описаний из раздела
    \ref{subsec:representation}
  \item Производит кластеризацию правил с помощью алгоритма
    \ref{algo:cluster}
  \item В зависимости от выбранного способа описания правил строит
    логические закономерности на основе результатов кластеризации.
  \end{enumerate}
\end{enumerate}

Для выполнения второй и третьей целей вычислительного эксперимента
написанный код был запущен на выборках, представленных в таблице
\ref{tab:data}. При этом использовалась следующая схема эксперимента:

\begin{enumerate}
  \item Исходная выборка делилась пополам на подвыборки обучения и
    контроля. При этом для подвыборок сохранялись одинаковые пропорции
    классов.
  \item На подвыборке обучения в системе <<Распознавание>>
    настраивался классификатор <<логические закономерности>>.
  \item Логические закономерности настроенного классификатора
    импортировались в программу, реализующую описанный подход к
    обработке множеств логических закономерностей.
  \item По этому набору логических закономерностей с помощью
    алгоритма <<простого голосования>> \cite{voron10logicalgs}
    проводилась классификация контрольной подвыборки.
  \item Далее проводилось описание логических закономерностей одним из
    двух способов из раздела \ref{subsec:representation}. Полученные
    описания проходили кластеризацию на \(t = 2, 3, \dots, n, \dots\)
    кластеров.
  \item По полученным на очередном шаге \(t\) центрам кластеров
    восстанавливались новые логические закономерности, которые затем
    использовались в алгоритме <<простого голосования>> для
    классификации контрольной подвыборки.
  \item Полученные значения доли правильно классифицированных объектов
    для разных способов представления и для разного числа кластеров
    \(t\) выносилось на график.
\end{enumerate}

Результаты запусков различных конфигураций метода обработки множества
логических закономерностей с помощью кластеризации представлены на
графиках \ref{fig:iris}, \ref{fig:wine}, \ref{fig:climate-new},
\ref{fig:climate-new2}, \ref{fig:ionosphere} и \ref{fig:ionosphere2}.
Сплошная линия красного цвета обозначает качество классификации
исходного множества логических закономерностей. Синяя линия с кружками
обозначает предложенный в данной работе подход, зеленая линия с
треугольниками и бирюзовая с квадратами --- подходы с бинаризованным
описанием логических закономерностей, построенных с помощью критериев
информативности IGain и Stat соответственно \cite{voron10logicalgs}.

Критерий информативности \(\text{IGain}\) --- это энтропийный критерий
информативности правила \(\varphi(x)\), отвечающего классу \(y_i\),
который имеет вид:

\begin{IEEEeqnarray}{rCl}\label{eq:igain}
\text{IGain} &=&
H\left(\frac{P}{P+N}, \frac{N}{P+N}\right)
- \frac{p + n}{P + N}
H\left(\frac{p}{p + n}, \frac{n}{p + n}\right) \nonumber \\
&-& \frac{P + N - p - n}{P + N}
H\left(\frac{P - p}{P + N - p - n}, \frac{N - n}{P + N - p - n}\right)
\end{IEEEeqnarray}

В формуле (\ref{eq:igain}) введено обозначение энтропии \(H(p, q) =
-p\log_2(p) -q\log_2(q)\), \(P, N\)~---~число объектов класса \(y_i\)
и не \(y_i\) соответственно, а \(p, n\)~---~число объектов из класса
\(y_i\) и не из класса \(y_i\), которые покрывает логическая
закономерность \(\varphi(x)\).

Критерий \(\text{Stat}\) имеет следующий вид:

\begin{IEEEeqnarray}{rCl}\label{eq:stat}
  \text{Stat} &=&
  -\ln \frac{C_{P_1}^{p_1}\dots C_{P_K}^{p_K}}{C_l^{p_1 + \dots + p_K}}
  \end{IEEEeqnarray}

В формуле (\ref{eq:stat}) используется обозначение
\(P_1, \dots, P_K\) --- количество объектов в классе \(1, \dots, K\)
и \(p_1, \dots, p_K\) --- количество объектов класса, покрываемых
закономерностью \(\varphi(x)\). Наконец,
\(C_n^k = \frac{n!}{k! (n-k)!}\) обозначает биномиальный коэффициент.

\paragraph{На выборках Iris и Wine}\label{par:toy}
(графики \ref{fig:iris} и \ref{fig:wine}) в обоих случаях способ
обработки множества логических закономерностей с помощью вектора левых
и правых границ, предложенный в данной работе, оказался заметно лучше
способа, изложенного в \cite{novikov15}. Оба графика показывают, что
обработка с помощью вектора левых и правых границ достигает качества
классификации исходного множества закономерностей, в то время как
методы из \cite{novikov15} имеют заметно худшее качество.

Из графика \ref{fig:iris} видно, что достаточно всего 20 логических
закономерностей вместо 29 исходных. В то же время график
\ref{fig:wine} говорит о том, что после обработки методом левых и
правых границ можно оставить всего 6 логических закономерностей вместо
исходных 36.

\paragraph{На выборке Climate}\label{par:clima}
(графики \ref{fig:climate-new} и \ref{fig:climate-new2}) наблюдается
противоположная картина. В случае чистых логических закономерностей
(график \ref{fig:climate-new}) имеется участок любопытно высокого
качества классификации для логических закономерностей, построенных с
помощью бинарного представления и критерия \(\text{IGain}\). С
увеличением числа логических закономерностей в кластере этот эффект
пропадает и подход из \cite{novikov15} становится сравним с подходом
из данной работы. Аналогичное поведение можно видеть на графике с
частичными закономерностями \ref{fig:climate-new2}, на котором
правила, построенные с помощью бинарного представления и критерия
\(\text{IGain}\), снова показывают лучшее качество классификации при
сравнительно малом числе логических закономерностей в кластере. Этот
эффект опять пропадает с ростом числа закономерностей в кластере и
методы из \cite{novikov15} становятся хуже метода из данной
работы. Стоит отметить, что в выборке Climate сильно
несбалансированные классы, что может быть причиной наблюдаемого
поведения.

В случае графика \ref{fig:climate-new} можно выбрать около 20--25
закономерностей вместо исходных 35 и добиться сравнимого качества
классификации. При этом в случае использования бинарного представления
и критерия \(\text{IGain}\) можно выбрать всего 4--9 правил. В то же
время для графика \ref{fig:climate-new} и подхода с вектором левых и
правых границ можно остановиться на 35 логических закономерностях
вместо исходных 63. При этом в случае использования бинарного
представления и критерия \(\text{IGain}\) это количество можно снизить
до 25.

\paragraph{На выборке Ionosphere}\label{par:ion}
(графики \ref{fig:ionosphere} и \ref{fig:ionosphere2}), в которой
классы также не являются сбалансированными, проводилась последняя
серия экспериментов. В случае, когда строились чистые логические
закономерности (график \ref{fig:ionosphere}), методы из
\cite{novikov15} показывают более хорошее качество классификации. С
другой стороны, при кластеризации частичных логических закономерностей
(график \ref{fig:ionosphere2}) уже метод левых и правых границ
демонстрирует лучшее качество классификации.

В случае графика \ref{fig:ionosphere} можно выбрать всего 1--2 правила
вместо исходных 48 и добиться качества классификации, которое заметно
выше исходного множества логических закономерностей. Это возможно
скорее всего потому, что данные легко разделимы. В то же время для
графика \ref{fig:ionosphere2} можно выбрать около 30 правил вместо
исходных 88 и при этом добиться качества классификации, которое
превосходит качество классификации с помощью исходного множества
логических закономерностей.


\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{0.8\textwidth}
  \centering
  \includegraphics
      [width=\textwidth,keepaspectratio]
      {iris}
      \caption{
        Выборка Iris. Анализ графика в разделе \ref{par:toy}.
      }
      \label{fig:iris}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
  \centering
  \includegraphics
      [width=\textwidth,keepaspectratio]
      {wine}
      \caption{
        Выборка Wine. Анализ графика в разделе \ref{par:toy}.
      }
      \label{fig:wine}
  \end{subfigure}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.8\textwidth}
  \includegraphics
      [width=\textwidth,keepaspectratio]
      {climate-new}
      \caption{
        Выборка Climate, исходные логические закономерности
        чистые. Анализ графика в разделе \ref{par:clima}.
      }
      \label{fig:climate-new}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics
      [width=\textwidth,keepaspectratio]
      {climate-new2}
      \caption{
        Выборка Climate, исходные логические закономерности
        частичные. Анализ графика в разделе \ref{par:clima}.
      }
      \label{fig:climate-new2}
  \end{subfigure}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.8\textwidth}
  \includegraphics
      [width=\textwidth,keepaspectratio]
      {ionosphere}
      \caption{
        Выборка Ionosphere, исходные логические закономерности
        чистые. Анализ графика в разделе \ref{par:ion}.
      }
      \label{fig:ionosphere}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics
      [width=\textwidth,keepaspectratio]
      {ionosphere2}
      \caption{
        Выборка Ionosphere, исходные логические закономерности
        частичные. Анализ графика в разделе \ref{par:ion}.
      }
      \label{fig:ionosphere2}
  \end{subfigure}
\end{figure}

\subsection{Обсуждение и выводы}
%% Приводятся выводы: в~какой степени результаты экспериментов
%% согласуются с~теорией?  Достигнут ли желаемый результат?  Обнаружены
%% ли какие-либо факты, не~нашедшие объяснения, и~которые нельзя
%% списать на «грязный» эксперимент?

%% Обсуждаются основные отличия предложенных методов от известных
%% ранее. В~чем их преимущества? Каковы границы их применимости? Какие
%% проблемы удалось решить, а~какие остались открытыми?  Какие возникли
%% новые постановки задач?

\begin{enumerate}
  \item С помощью обработки множеств логических закономерностей
    удается получить множество меньшей мощности, которое проще
    интерпретировать.
  \item Обработка логических закономерностей, использующая описание
    правил с помощью вектора левых и правых границ, строит логические
    закономерности, которые в целом показывают результаты как лучше
    (\ref{fig:iris}, \ref{fig:wine}, \ref{fig:ionosphere2}), так и
    хуже (\ref{fig:ionosphere}), чем результаты обработки с
    использованием бинаризованных описаний правил из
    \cite{novikov15}. Также есть случаи, в которых поведение двух
    подходов примерно одинаково: \ref{fig:climate-new},
    \ref{fig:climate-new2}.
  \item Обработка с использованием бинаризованных представлений правил
    иногда работает лучше \ref{fig:ionosphere}. Если же исходно
    используются не чистые, а частичные закономерности, то
    представление с помощью вектора левых и правых границ более
    устойчиво (\ref{fig:climate-new2}, \ref{fig:ionosphere2}) в том
    плане, что качество классификации оказывается сравнимым с исходным
    множеством логических закономерностей.
  \item В результате обработки множества логических закономерностей
    как с помощью бинаризованного представления, так и с помощью
    вектора левых и правых границ, можно получить сравнимое с исходным
    качество классификации при меньшем количестве использованных
    правил.
\end{enumerate}

\section{Заключение}

%% В~квалификационных работах последний раздел нужен для того, чтобы
%% конспективно перечислить основные результаты, полученные лично
%% автором.

%% Результатами, в~частности, являются:
%% \begin{itemize}
%% \item
%%     Предложен новый подход к\dots
%% \item
%%     Разработан новый метод\dots, позволяющий\dots
%% \item
%%     Доказан ряд теорем, подтверждающих (опровергающих), что\dots
%% \item
%%     Проведены вычислительные эксперименты\dots, которые подтвердили /
%%     опровергли / привели к~новым постановкам задач.
%% \end{itemize}

%% Цель данного раздела: доказать квалификацию автора.  Даже беглого
%% взгляда на заключение должно быть достаточно, чтобы стало ясно:
%% автору удалось решить актуальную, трудную, ранее не~решённую задачу,
%% предложенные автором решения обоснованы и~проверены.

%% Иногда в~Заключении приводится список направлений дальнейших
%% исследований.

\begin{enumerate}
  \item В данной работе был предложен подход к обработке множеств
    логических закономерностей с помощью кластеризации на основе
    дисперсионного критерия. В разделе \ref{sec:processing} была
    сформулирована общая схема подхода, а в разделе
    \ref{sec:experiments} были поставлены эксперименты по его
    исследованию.
  \item Получены обработанные множества логических закономерностей для
    задач классификации выборок Iris, Wine, Climate, Ionosphere.
  \item Проведено сравнение метода обработки с использованием вектора
    левых и правых границ с методом обработки, в котором используется
    бинаризованное описание логических закономерностей из
    \cite{novikov15}
  \item На практических задачах показано, что использование
    кластеризации при обработке множества логических закономерностей
    позволяет получить меньшее количество правил. При этом
    классификатор по схеме простого голосования имеет качество,
    сравнимое с исходным алгоритмом из системы <<Распознавание>>.
\end{enumerate}

\newpage
\appendix
\section{Система <<Распознавание>>}
Система <<Распознавание>> \cite{recognition06} работает под
управлением операционной системы семейства Microsoft Windows XP и
выше. В этом приложении будет показано, как систему <<Распознавание>>
можно использовать на UNIX-подобных операционных системах. Для этого
нужно:

\begin{enumerate}
  \item Установить свободное программное обеспечение
    \href{https://www.winehq.org/}{Wine}.
  \item Настроить 32-битный префикс командой
    \mint{shell}|WINEARCH=win32 WINEPREFIX=~/.wine32 winecfg|
  \item Установить в созданный префикс недостающую библиотеку
    \mint{shell}|WINEPREFIX=~/.wine32 winetricks mfc42|
  \item Запустить систему <<Распознавание>> командой
    \begin{minted}{shell}
      LC_CTYPE=ru_RU.utf8 WINEARCH=win32 WINEPREFIX=~/.wine32 \
      wine Recognition.exe
    \end{minted}
\end{enumerate}

\section{Формат файлов *.tab}

Система <<Распознавание>> \cite{recognition06} использует собственный
формат файлов описания выборки. Здесь приведено описание этого
формата:

Первая строка --- заголовок. Представляет из себя числа, разделенные
пробелом. Первое число --- количество признаков каждого объекта
выборки. Второе --- количество классов. Далее идет число \(0\), за
которым идет число объектов первого класса, затем сумма числа объектов
первых двух классов, затем сумма числа объектов первых трех классов и
так далее. Завершает заголовок число, обозначающее пропуск измерения в
данных.

Каждая последующая строка содержит признаковое описание отдельного
объекта, разные классы разделены пустой строкой.

\printbibliography

\end{document}
